{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec776c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'상품': '35161_농심)신라면큰사발면114G', '정확도': 0.5}]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import glob     \n",
    "import json\n",
    "import os\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "\n",
    "# 1. 모델 불러오기\n",
    "model = tf.keras.models.load_model(r'C:\\Users\\sj990\\MachineLearning\\CAPSTONE_DESIGN_AI_Module\\temporary\\면종류\\model_trained_in_NEURON_v2.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "# 경로설정\n",
    "image_directory = r'C:\\Users\\sj990\\Desktop\\ex3'\n",
    "\n",
    "# jpg, png, jpeg 형식 (리스트형식으로 저장된다, 경로에는 파일한장만 존재하게)\n",
    "image_files = glob.glob(image_directory + '/*.png') + glob.glob(image_directory + '/*.jpg') + glob.glob(image_directory + '/*.jpeg')\n",
    "\n",
    "# 리스트중 첫번째것만 사용\n",
    "image_path = image_files[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 이미지전처리, 사이즈조정만\n",
    "img = tf.keras.preprocessing.image.load_img(image_path, target_size=(256, 256))\n",
    "img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img = img.astype('float32') / 255.0\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "# 이미지 예측\n",
    "preds = model.predict(img)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class_labels = []\n",
    "\n",
    "# 3. txt파일 불러오기\n",
    "class_labels_path = r'C:\\Users\\sj990\\MachineLearning\\CAPSTONE_DESIGN_AI_Module\\temporary\\면종류\\class_labels_noodle.txt'\n",
    "\n",
    "with open(class_labels_path, 'r') as file:\n",
    "    for line in file:\n",
    "        class_labels.append(line.strip())\n",
    "\n",
    "\n",
    "# 결과값 상위 1개\n",
    "top_preds_idx = preds[0].argsort()[ : : -1][ : 1]\n",
    "top_preds_labels = [class_labels[idx] for idx in top_preds_idx]\n",
    "top_preds_probs = preds[0][top_preds_idx]\n",
    "\n",
    "results = []\n",
    "\n",
    "#json형식 results\n",
    "for label, prob in zip(top_preds_labels, top_preds_probs):\n",
    "    result = {\n",
    "        '상품': label,\n",
    "        '정확도': round(float(prob), 2)\n",
    "    }\n",
    "    results.append(result) \n",
    "\n",
    "\n",
    "#출력예시\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d389d382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dc1d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b279bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e645eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a52bf14b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 70\u001b[0m\n\u001b[0;32m     67\u001b[0m top_preds_labels \u001b[38;5;241m=\u001b[39m [class_labels[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m top_preds_idx]\n\u001b[0;32m     68\u001b[0m top_preds_probs \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m0\u001b[39m][top_preds_idx]\n\u001b[1;32m---> 70\u001b[0m \u001b[43mtop_preds_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[0;32m     72\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m#json형식 results\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import glob     \n",
    "import json\n",
    "import os\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "\n",
    "# 1. 모델 불러오기\n",
    "model = tf.keras.models.load_model(r'C:\\Users\\sj990\\MachineLearning\\CAPSTONE_DESIGN_AI_Module\\temporary\\미분류\\미분류모델.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "# 경로설정\n",
    "image_directory = r'C:\\Users\\sj990\\Desktop\\ex3'\n",
    "\n",
    "# jpg, png, jpeg 형식 (리스트형식으로 저장된다, 경로에는 파일한장만 존재하게)\n",
    "image_files = glob.glob(image_directory + '/*.png') + glob.glob(image_directory + '/*.jpg') + glob.glob(image_directory + '/*.jpeg')\n",
    "\n",
    "# 리스트중 첫번째것만 사용\n",
    "image_path = image_files[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 이미지전처리, 사이즈조정만\n",
    "img = tf.keras.preprocessing.image.load_img(image_path, target_size=(256, 256))\n",
    "img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img = img.astype('float32') / 255.0\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "# 이미지 예측\n",
    "preds = model.predict(img)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class_labels = []\n",
    "\n",
    "# 3. txt파일 불러오기\n",
    "class_labels_path = r'C:\\Users\\sj990\\MachineLearning\\CAPSTONE_DESIGN_AI_Module\\temporary\\미분류\\미분류라벨.txt'\n",
    "\n",
    "with open(class_labels_path, 'r') as file:\n",
    "    for line in file:\n",
    "        class_labels.append(line.strip())\n",
    "\n",
    "\n",
    "# 결과값 상위 1개\n",
    "top_preds_idx = preds[0].argsort()[ : : -1][ : 1]\n",
    "top_preds_labels = [class_labels[idx] for idx in top_preds_idx]\n",
    "top_preds_probs = preds[0][top_preds_idx]\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "#json형식 results\n",
    "for label, prob in zip(top_preds_labels, top_preds_probs):\n",
    "    result = {\n",
    "        '상품': label,\n",
    "        '정확도': round(float(prob), 2)\n",
    "    }\n",
    "    results.append(result)\n",
    "        \n",
    "        \n",
    "'''\n",
    "# json파일 저장필요할시 주석 제거후 사용\n",
    "with open('results.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(results, file, ensure_ascii=False)\n",
    "'''\n",
    "\n",
    "\n",
    "#출력예시\n",
    "print(results)\n",
    "\n",
    "'''\n",
    "# 사용한 이미지 파일 삭제 필요할시 주석 제거후 사용\n",
    "if results != None :\n",
    "    os.remove(image_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2689b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17738e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['70157_크린랩)크린지퍼백(이중지퍼백)25cmX30cm'] [0.5106692]\n",
      "['70157_크린랩)크린지퍼백(이중지퍼백)25cmX30cm'] [0.46854502]\n",
      "['60093_소피)귀애랑중형'] [0.4914779]\n",
      "['70157_크린랩)크린지퍼백(이중지퍼백)25cmX30cm'] [0.7544632]\n",
      "['70157_크린랩)크린지퍼백(이중지퍼백)25cmX30cm'] [0.5535329]\n",
      "['25097_브레드가든컬러슈가파우더60G'] [0.41363204]\n",
      "['45432_한국라이스텍)찹쌀'] [0.65328825]\n",
      "['90091_소피바디피트천연코튼팬티라이너라벤더20개입'] [0.46301022]\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['90128_오뚜기참깨라면용기110G'] [0.9919905]\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10103_오뚜기참깨라면(컵)'] [0.9385439]\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['90128_오뚜기참깨라면용기110G'] [0.5288172]\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['90128_오뚜기참깨라면용기110G'] [0.93525565]\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['90128_오뚜기참깨라면용기110G'] [0.9916221]\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10101_농심오징어짬뽕컵67G'] [0.4113204]\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['15054_삼양짜장불닭볶음면105G'] [0.40149915]\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (1, 256, 256, 3) for input KerasTensor(type_spec=TensorSpec(shape=(1, 256, 256, 3), dtype=tf.float32, name='keras_layer_18_input'), name='keras_layer_18_input', description=\"created by layer 'keras_layer_18_input'\"), but it was called on an input with incompatible shape (None, 128, 128, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (1, 256, 256, 3) for input KerasTensor(type_spec=TensorSpec(shape=(1, 256, 256, 3), dtype=tf.float32, name='keras_layer_18_input'), name='keras_layer_18_input', description=\"created by layer 'keras_layer_18_input'\"), but it was called on an input with incompatible shape (None, 128, 128, 3).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['90128_오뚜기참깨라면용기110G'] [0.9549429]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import glob     \n",
    "import json\n",
    "import os\n",
    "from PIL import Image, ImageFilter\n",
    "from skimage import exposure\n",
    "import cv2\n",
    "\n",
    "\n",
    "class Model:\n",
    "    # 초기화\n",
    "    def __init__(self, img_path, model_path, class_labels_path):\n",
    "        self.img_path = img_path\n",
    "        self.model_path = model_path\n",
    "        self.class_labels_path = class_labels_path\n",
    "        \n",
    "        image_directory = img_path\n",
    "        # jpg, png, jpeg 형식 (리스트형식으로 저장된다, 경로에는 파일한장만 존재하게)\n",
    "        image_files = glob.glob(image_directory + '/*.png') + glob.glob(image_directory + '/*.jpg') + glob.glob(image_directory + '/*.jpeg')\n",
    "        # 리스트중 첫번째것만 사용\n",
    "        self.image_path = image_files[0]\n",
    "        \n",
    "        \n",
    "    #예측 (전처리된이미지와 클래스라벨을 삽입)\n",
    "    def preds_model(self, img, class_labels):\n",
    "        model = tf.keras.models.load_model(self.model_path,  custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "        preds = model.predict(img) \n",
    "        \n",
    "        # 결과값 상위 1개\n",
    "        top_preds_idx = preds[0].argsort()[ : : -1][ : 1]\n",
    "        top_preds_labels = [class_labels[idx] for idx in top_preds_idx]\n",
    "        top_preds_probs = preds[0][top_preds_idx]\n",
    "        print(top_preds_labels, top_preds_probs)\n",
    "    \n",
    "    #이미지전처리_1 (기본)\n",
    "    def img_pre_default(self):  \n",
    "        img = tf.keras.preprocessing.image.load_img(self.image_path, target_size=(256, 256))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img.astype('float32') / 255.0\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img\n",
    "    \n",
    "    #이미지전처리_2 (가우시안)\n",
    "    def img_pre_gaussian(self): \n",
    "        img = Image.open(self.image_path)\n",
    "        img = img.filter(ImageFilter.GaussianBlur(radius=4))\n",
    "        img = img.resize((256, 256))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img.astype('float32') / 255.0\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img\n",
    "    \n",
    "    #이미지전처리_3 (히스토그램 평활화)\n",
    "    def img_pre_hist(self):\n",
    "        img = tf.keras.preprocessing.image.load_img(self.image_path, target_size=(256, 256))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img.astype('float32') / 255.0\n",
    "        img = exposure.equalize_hist(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img\n",
    "    \n",
    "    #이미지전처리_4 (데이터증강)\n",
    "    def img_pre_augmentation(self): \n",
    "        img = tf.keras.preprocessing.image.load_img(self.image_path, target_size=(256, 256))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img.astype('float32') / 255.0\n",
    "        \n",
    "        # 데이터 증강을 위한 ImageDataGenerator 생성\n",
    "        train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            rescale=1.0 / 255,\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.4,\n",
    "            height_shift_range=0.4,\n",
    "            shear_range=0.4,\n",
    "            zoom_range=0.4,\n",
    "            horizontal_flip=True,\n",
    "            preprocessing_function=lambda img: tf.image.random_crop(img, [256, 256, 3])\n",
    "        )\n",
    "        \n",
    "        # 데이터 증강 적용\n",
    "        img = train_datagen.random_transform(img)\n",
    "        \n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img\n",
    "    \n",
    "    \n",
    "    #이미지전처리_5 (이미지회전)\n",
    "    def img_pre_rotate(self):\n",
    "        img = tf.keras.preprocessing.image.load_img(self.image_path, target_size=(256, 256))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img.astype('float32') / 255.0\n",
    "\n",
    "        # 회전 각도 설정\n",
    "        angle = 75\n",
    "\n",
    "        # 이미지 회전\n",
    "        img = tf.keras.preprocessing.image.random_rotation(img, angle, row_axis=0, col_axis=1, channel_axis=2)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img    \n",
    "    \n",
    "    \n",
    "    #이미지전처리_6 (밝기조절)\n",
    "    def img_pre_adjust_brightness(self):\n",
    "        img = tf.keras.preprocessing.image.load_img(self.image_path, target_size=(256, 256))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img.astype('float32') / 255.0\n",
    "\n",
    "        # 밝기 조절\n",
    "        brightness_factor = 0.4\n",
    "        img = tf.image.adjust_brightness(img, delta=brightness_factor)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img\n",
    "    \n",
    "    #이미지전처리_7 (대비조절)\n",
    "    def img_pre_adjust_contrast(self):\n",
    "        img = tf.keras.preprocessing.image.load_img(self.image_path, target_size=(256, 256))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img.astype('float32') / 255.0\n",
    "\n",
    "        # 대비 조절\n",
    "        contrast_factor = 3.0\n",
    "        img = tf.image.adjust_contrast(img, contrast_factor)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img\n",
    "    \n",
    "    # 이미지전처리_8 (이미지 확대 및 축소)\n",
    "    def img_pre_resize(self):\n",
    "        img = tf.keras.preprocessing.image.load_img(self.image_path, target_size=(256, 256))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img.astype('float32') / 255.0\n",
    "\n",
    "        # 이미지 크기 변경\n",
    "        target_size = (128, 128)\n",
    "        img = tf.image.resize(img, target_size)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    #라벨불러오기\n",
    "    def class_label(self):\n",
    "        class_labels = []\n",
    "        with open(self.class_labels_path, 'r') as file:\n",
    "            for line in file:\n",
    "                class_labels.append(line.strip())\n",
    "        return class_labels\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "model1 = Model(r'C:\\Users\\sj990\\Desktop\\ex3',\n",
    "               r'C:\\Users\\sj990\\MachineLearning\\CAPSTONE_DESIGN_AI_Module\\temporary\\미분류\\미분류모델.h5',\n",
    "               r'C:\\Users\\sj990\\MachineLearning\\CAPSTONE_DESIGN_AI_Module\\temporary\\미분류\\미분류라벨.txt')\n",
    "\n",
    "model2 = Model(r'C:\\Users\\sj990\\Desktop\\ex3',\n",
    "               r'C:\\Users\\sj990\\MachineLearning\\CAPSTONE_DESIGN_AI_Module\\temporary\\면종류\\model_trained_in_NEURON_v2.h5',\n",
    "               r'C:\\Users\\sj990\\MachineLearning\\CAPSTONE_DESIGN_AI_Module\\temporary\\면종류\\class_labels_noodle.txt')\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "class_labels_1 = model1.class_label()\n",
    "\n",
    "'''\n",
    "for i in range(10):\n",
    "    model1_pre_img = model1.img_pre_augmentation()\n",
    "    result4 = model1.preds_model(model1_pre_img, class_labels_1)\n",
    "'''\n",
    "\n",
    "model1_default_img = model1.img_pre_default()\n",
    "model1_gaussian_img = model1.img_pre_gaussian()\n",
    "model1_hist_img = model1.img_pre_hist()\n",
    "model1_pre_img = model1.img_pre_augmentation()\n",
    "model1_rotate_img = model1.img_pre_rotate()\n",
    "model1_bright_img = model1.img_pre_adjust_brightness()\n",
    "model1_contrast_img = model1.img_pre_adjust_contrast()\n",
    "model1_resize_img = model1.img_pre_resize()\n",
    "\n",
    "result1 = model1.preds_model(model1_default_img, class_labels_1)\n",
    "result2 = model1.preds_model(model1_gaussian_img, class_labels_1)\n",
    "result3 = model1.preds_model(model1_hist_img, class_labels_1)\n",
    "result4 = model1.preds_model(model1_pre_img, class_labels_1)\n",
    "result5 = model1.preds_model(model1_rotate_img, class_labels_1)\n",
    "result6 = model1.preds_model(model1_bright_img, class_labels_1)\n",
    "result7 = model1.preds_model(model1_contrast_img, class_labels_1)\n",
    "result8 = model1.preds_model(model1_resize_img, class_labels_1)\n",
    "\n",
    "\n",
    "\n",
    "class_labels_2 = model2.class_label()\n",
    "\n",
    "'''\n",
    "for i in range(10):\n",
    "    model2_pre_img = model2.img_pre_augmentation()\n",
    "    result4 = model2.preds_model(model1_pre_img, class_labels_2)    \n",
    "''' \n",
    "\n",
    "model2_default_img = model2.img_pre_default()\n",
    "model2_gaussian_img = model2.img_pre_gaussian()\n",
    "model2_hist_img = model2.img_pre_hist()\n",
    "model2_pre_img = model2.img_pre_augmentation()\n",
    "model2_rotate_img=model2.img_pre_rotate()\n",
    "model2_bright_img=model2.img_pre_adjust_brightness()\n",
    "model2_contrast_img = model2.img_pre_adjust_contrast()\n",
    "model2_resize_img = model2.img_pre_resize()\n",
    "\n",
    "\n",
    "result1 = model2.preds_model(model2_default_img, class_labels_2)\n",
    "result2 = model2.preds_model(model2_gaussian_img, class_labels_2)\n",
    "result3 = model2.preds_model(model2_hist_img, class_labels_2)\n",
    "result4 = model2.preds_model(model2_pre_img, class_labels_2)\n",
    "result5 = model2.preds_model(model2_rotate_img, class_labels_2)\n",
    "result6 = model2.preds_model(model1_bright_img, class_labels_2)\n",
    "result7 = model2.preds_model(model2_contrast_img, class_labels_2)\n",
    "result8 = model2.preds_model(model1_resize_img, class_labels_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821e647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af63418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91373cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'상품': '80079_바디밸런스)맨스언더웨어남스판드로즈', '정확도': 1.0}]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import glob     \n",
    "import json\n",
    "import os\n",
    "from PIL import Image, ImageFilter\n",
    "from skimage import exposure\n",
    "import cv2\n",
    "\n",
    "\n",
    "class Model:\n",
    "    # 초기화\n",
    "    def __init__(self, img_path, model_path, class_labels_path):\n",
    "        self.img_path = img_path\n",
    "        self.model_path = model_path\n",
    "        self.class_labels_path = class_labels_path\n",
    "        \n",
    "        image_directory = img_path\n",
    "        # jpg, png, jpeg 형식 (리스트형식으로 저장된다, 경로에는 파일한장만 존재하게)\n",
    "        image_files = glob.glob(image_directory + '/*.png') + glob.glob(image_directory + '/*.jpg') + glob.glob(image_directory + '/*.jpeg')\n",
    "        # 리스트중 첫번째것만 사용\n",
    "        self.image_path = image_files[0]\n",
    "        \n",
    "        \n",
    "    #예측 (전처리된이미지와 클래스라벨을 삽입)\n",
    "    def preds_model(self, img, class_labels):\n",
    "        model = tf.keras.models.load_model(self.model_path,  custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "        preds = model.predict(img) \n",
    "        \n",
    "        # 결과값 상위 1개\n",
    "        top_preds_idx = preds[0].argsort()[ : : -1][ : 1]\n",
    "        top_preds_labels = [class_labels[idx] for idx in top_preds_idx]\n",
    "        top_preds_probs = preds[0][top_preds_idx]\n",
    "        return top_preds_labels, top_preds_probs\n",
    "    \n",
    "    #이미지전처리_1 (기본)\n",
    "    def img_pre_default(self):  \n",
    "        img = tf.keras.preprocessing.image.load_img(self.image_path, target_size=(256, 256))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img.astype('float32') / 255.0\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img\n",
    "    \n",
    "    \"\"\"\n",
    "    #이미지전처리_2 (가우시안 필터)\n",
    "    def img_pre_gaussian(self): \n",
    "        img = Image.open(self.image_path)\n",
    "        img = img.filter(ImageFilter.GaussianBlur(radius=4))\n",
    "        img = img.resize((256, 256))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img.astype('float32') / 255.0\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img\n",
    "    \n",
    "    #이미지전처리_3 (히스토그램 평활화)\n",
    "    def img_pre_hist(self):\n",
    "        img = tf.keras.preprocessing.image.load_img(self.image_path, target_size=(256, 256))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img.astype('float32') / 255.0\n",
    "        img = exposure.equalize_hist(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img\n",
    "    \n",
    "    #이미지전처리_4 (데이터증강)\n",
    "    def img_pre_augmentation(self): \n",
    "        img = tf.keras.preprocessing.image.load_img(self.image_path, target_size=(256, 256))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img.astype('float32') / 255.0\n",
    "        \n",
    "        # 데이터 증강을 위한 ImageDataGenerator 생성\n",
    "        train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            rescale=1.0 / 255,\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.4,\n",
    "            height_shift_range=0.4,\n",
    "            shear_range=0.4,\n",
    "            zoom_range=0.4,\n",
    "            horizontal_flip=True,\n",
    "            preprocessing_function=lambda img: tf.image.random_crop(img, [256, 256, 3])\n",
    "        )\n",
    "        \n",
    "        # 데이터 증강 적용\n",
    "        img = train_datagen.random_transform(img)\n",
    "        \n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img\n",
    "    \n",
    "    \n",
    "    #이미지전처리_5 (이미지회전)\n",
    "    def img_pre_rotate(self):\n",
    "        img = tf.keras.preprocessing.image.load_img(self.image_path, target_size=(256, 256))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img.astype('float32') / 255.0\n",
    "\n",
    "        # 회전 각도 설정\n",
    "        angle = 75\n",
    "\n",
    "        # 이미지 회전\n",
    "        img = tf.keras.preprocessing.image.random_rotation(img, angle, row_axis=0, col_axis=1, channel_axis=2)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img    \n",
    "    \n",
    "    \n",
    "    #이미지전처리_6 (밝기조절)\n",
    "    def img_pre_adjust_brightness(self):\n",
    "        img = tf.keras.preprocessing.image.load_img(self.image_path, target_size=(256, 256))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img.astype('float32') / 255.0\n",
    "\n",
    "        # 밝기 조절\n",
    "        brightness_factor = 0.4\n",
    "        img = tf.image.adjust_brightness(img, delta=brightness_factor)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img\n",
    "    \n",
    "    #이미지전처리_7 (대비조절)\n",
    "    def img_pre_adjust_contrast(self):\n",
    "        img = tf.keras.preprocessing.image.load_img(self.image_path, target_size=(256, 256))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img.astype('float32') / 255.0\n",
    "\n",
    "        # 대비 조절\n",
    "        contrast_factor = 3.0\n",
    "        img = tf.image.adjust_contrast(img, contrast_factor)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img\n",
    "    \n",
    "    # 이미지전처리_8 (이미지 확대 및 축소)\n",
    "    def img_pre_resize(self):\n",
    "        img = tf.keras.preprocessing.image.load_img(self.image_path, target_size=(256, 256))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img.astype('float32') / 255.0\n",
    "\n",
    "        # 이미지 크기 변경\n",
    "        target_size = (128, 128)\n",
    "        img = tf.image.resize(img, target_size)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img\n",
    "    \"\"\"\n",
    "\n",
    "    #라벨불러오기\n",
    "    def class_label(self):\n",
    "        class_labels = []\n",
    "        with open(self.class_labels_path, 'r') as file:\n",
    "            for line in file:\n",
    "                class_labels.append(line.strip())\n",
    "        return class_labels\n",
    "    \n",
    "    #결과출력\n",
    "    def print_result(self, top_preds_labels, top_preds_probs):\n",
    "        results = []\n",
    "        #json형식 results\n",
    "        for label, prob in zip(top_preds_labels, top_preds_probs):\n",
    "            result = {\n",
    "                '상품': label,\n",
    "                '정확도': round(float(prob), 2)\n",
    "            }\n",
    "            results.append(result)\n",
    "        print(results)\n",
    "        return results\n",
    "  \n",
    "################################경로설정################################\n",
    "\n",
    "\n",
    "# 이미지들어있는 폴더경로 \n",
    "image_path = r'C:\\Users\\sj990\\Desktop\\ex4'\n",
    "\n",
    "#분류기 모델경로, 분류기 라벨경로\n",
    "clsf_model_path = r'C:\\Users\\sj990\\Desktop\\모델,라벨\\모델\\classification.h5'\n",
    "clsf_label_path = r'C:\\Users\\sj990\\Desktop\\모델,라벨\\라벨\\classification_label.txt'\n",
    "\n",
    "# 면류 모델경로, 면류 라벨경로\n",
    "noodle_model_path = r'C:\\Users\\sj990\\Desktop\\모델,라벨\\모델\\model_noodle.h5'\n",
    "noodle_label_path = r'C:\\Users\\sj990\\Desktop\\모델,라벨\\라벨\\model_noodle_label.txt'\n",
    "\n",
    "# 미분류 모델경로, 면류 라벨경로\n",
    "unclassified_model_path = r'C:\\Users\\sj990\\Desktop\\모델,라벨\\모델\\model_unclass.h5'\n",
    "unclassified_label_path = r'C:\\Users\\sj990\\Desktop\\모델,라벨\\라벨\\model_unclass_label.txt'\n",
    "\n",
    "################################경로설정################################\n",
    "\n",
    "### 모델분류 ###\n",
    "model_classification = Model(image_path, clsf_model_path, clsf_label_path)\n",
    "class_labels = model_classification.class_label()\n",
    "model_default_img = model_classification.img_pre_default()\n",
    "classification_label, classification_prob = model_classification.preds_model(model_default_img, class_labels)\n",
    "\n",
    "\n",
    "### 특징분류 ###\n",
    "if(classification_label[0] == \"라면\"):\n",
    "    #모델정의\n",
    "    model_noodle = Model(image_path, noodle_model_path, noodle_label_path)\n",
    "    #예측\n",
    "    result_noodle_labels, result_noodle_probs = model_noodle.preds_model(model_noodle.img_pre_default(), model_noodle.class_label())\n",
    "    #결과출력\n",
    "    noodle_result = model_noodle.print_result(result_noodle_labels, result_noodle_probs)\n",
    "     \n",
    "    # json파일 저장필요할시 주석 제거후 사용\n",
    "    with open('results_noodle.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(noodle_result, file, ensure_ascii=False)   \n",
    "    \n",
    "    '''\n",
    "    # 사용한 이미지 파일 삭제 필요할시 주석 제거후 사용\n",
    "    if results != None :\n",
    "        os.remove(image_path)\n",
    "    '''\n",
    "    \n",
    "elif(classification_label[0] ==\"미분류\"):\n",
    "    #모델정의\n",
    "    model_unclassified = Model(image_path, unclassified_model_path, unclassified_label_path)\n",
    "    #예측\n",
    "    result_unclassified_labels, result_unclassified_probs = model_unclassified.preds_model(model_unclassified.img_pre_default(), model_unclassified.class_label())\n",
    "    #결과출력\n",
    "    unclassified_result = model_unclassified.print_result(result_unclassified_labels, result_unclassified_probs)\n",
    "    \n",
    "    \n",
    "    # json파일 저장필요할시 주석 제거후 사용\n",
    "    with open('results_unclassified.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(unclassified_result, file, ensure_ascii=False)\n",
    "      \n",
    "    '''\n",
    "    # 사용한 이미지 파일 삭제 필요할시 주석 제거후 사용\n",
    "    if results != None :\n",
    "        os.remove(image_path)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e28df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e17ed3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Found 23370 images belonging to 205 classes.\n",
      "Found 3075 images belonging to 205 classes.\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 96\u001b[0m\n\u001b[0;32m     92\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     94\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m LearningRateScheduler(lr_schedule)\n\u001b[1;32m---> 96\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m    105\u001b[0m best_val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py:1134\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1128\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_coordinator \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mcoordinator\u001b[38;5;241m.\u001b[39mClusterCoordinator(\n\u001b[0;32m   1129\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), \\\n\u001b[0;32m   1132\u001b[0m      training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1133\u001b[0m   \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1134\u001b[0m   data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m      \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m      \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m      \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m      \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1150\u001b[0m   \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\data_adapter.py:1383\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1382\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\data_adapter.py:1138\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1135\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution_value \u001b[38;5;241m=\u001b[39m steps_per_execution\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m   1137\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1152\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\data_adapter.py:917\u001b[0m, in \u001b[0;36mKerasSequenceAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keras_sequence \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enqueuer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m \u001b[38;5;28msuper\u001b[39m(KerasSequenceAdapter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    918\u001b[0m     x,\n\u001b[0;32m    919\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# Shuffle is handed in the _make_callable override.\u001b[39;00m\n\u001b[0;32m    920\u001b[0m     workers\u001b[38;5;241m=\u001b[39mworkers,\n\u001b[0;32m    921\u001b[0m     use_multiprocessing\u001b[38;5;241m=\u001b[39muse_multiprocessing,\n\u001b[0;32m    922\u001b[0m     max_queue_size\u001b[38;5;241m=\u001b[39mmax_queue_size,\n\u001b[0;32m    923\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\data_adapter.py:794\u001b[0m, in \u001b[0;36mGeneratorDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28msuper\u001b[39m(GeneratorDataAdapter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    792\u001b[0m \u001b[38;5;66;03m# Since we have to know the dtype of the python generator when we build the\u001b[39;00m\n\u001b[0;32m    793\u001b[0m \u001b[38;5;66;03m# dataset, we have to look at a batch to infer the structure.\u001b[39;00m\n\u001b[1;32m--> 794\u001b[0m peek, x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_peek_and_restore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    795\u001b[0m peek \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_standardize_batch(peek)\n\u001b[0;32m    796\u001b[0m peek \u001b[38;5;241m=\u001b[39m _process_tensorlike(peek)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\data_adapter.py:928\u001b[0m, in \u001b[0;36mKerasSequenceAdapter._peek_and_restore\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_peek_and_restore\u001b[39m(x):\n\u001b[1;32m--> 928\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py:65\u001b[0m, in \u001b[0;36mIterator.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_index_array()\n\u001b[0;32m     63\u001b[0m index_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_array[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m*\u001b[39m idx:\n\u001b[0;32m     64\u001b[0m                                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m*\u001b[39m (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batches_of_transformed_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py:239\u001b[0m, in \u001b[0;36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    237\u001b[0m         params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_data_generator\u001b[38;5;241m.\u001b[39mget_random_transform(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    238\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_data_generator\u001b[38;5;241m.\u001b[39mapply_transform(x, params)\n\u001b[1;32m--> 239\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_data_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstandardize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m     batch_x[i] \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    241\u001b[0m \u001b[38;5;66;03m# optionally save augmented images to disk for debugging purposes\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:708\u001b[0m, in \u001b[0;36mImageDataGenerator.standardize\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"Applies the normalization configuration in-place to a batch of inputs.\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \n\u001b[0;32m    693\u001b[0m \u001b[38;5;124;03m`x` is changed in-place since the function is mainly used internally\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;124;03m    The inputs, normalized.\u001b[39;00m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessing_function:\n\u001b[1;32m--> 708\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale:\n\u001b[0;32m    710\u001b[0m     x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale\n",
      "Cell \u001b[1;32mIn[5], line 42\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     28\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# 데이터증강기법\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     train_datagen \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mImageDataGenerator(\n\u001b[0;32m     34\u001b[0m         rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m,\n\u001b[0;32m     35\u001b[0m         rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     36\u001b[0m         width_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m     37\u001b[0m         height_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m     38\u001b[0m         shear_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m     39\u001b[0m         zoom_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m     40\u001b[0m         horizontal_flip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m         fill_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m---> 42\u001b[0m         preprocessing_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m img: \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     )\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# 훈련 데이터셋\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     train_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:/종설/Training/1/[원천]면류\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m    208\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    209\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py:400\u001b[0m, in \u001b[0;36mrandom_crop\u001b[1;34m(value, size, seed, name)\u001b[0m\n\u001b[0;32m    398\u001b[0m value \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(value, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    399\u001b[0m size \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(size, dtype\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mint32, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 400\u001b[0m shape \u001b[38;5;241m=\u001b[39m \u001b[43marray_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    401\u001b[0m check \u001b[38;5;241m=\u001b[39m control_flow_ops\u001b[38;5;241m.\u001b[39mAssert(\n\u001b[0;32m    402\u001b[0m     math_ops\u001b[38;5;241m.\u001b[39mreduce_all(shape \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m size),\n\u001b[0;32m    403\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeed value.shape >= size, got \u001b[39m\u001b[38;5;124m\"\u001b[39m, shape, size],\n\u001b[0;32m    404\u001b[0m     summarize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m    405\u001b[0m shape \u001b[38;5;241m=\u001b[39m control_flow_ops\u001b[38;5;241m.\u001b[39mwith_dependencies([check], shape)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m    208\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    209\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:651\u001b[0m, in \u001b[0;36mshape\u001b[1;34m(input, name, out_type)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    628\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshape\u001b[39m(\u001b[38;5;28minput\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out_type\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mint32):\n\u001b[0;32m    630\u001b[0m   \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[0;32m    631\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns the shape of a tensor.\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \n\u001b[0;32m    633\u001b[0m \u001b[38;5;124;03m  This operation returns a 1-D integer tensor representing the shape of `input`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;124;03m    A `Tensor` of type `out_type`.\u001b[39;00m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 651\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshape_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:679\u001b[0m, in \u001b[0;36mshape_internal\u001b[1;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[0;32m    677\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m optimize \u001b[38;5;129;01mand\u001b[39;00m input_shape\u001b[38;5;241m.\u001b[39mis_fully_defined():\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m constant(input_shape\u001b[38;5;241m.\u001b[39mas_list(), out_type, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m--> 679\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:9182\u001b[0m, in \u001b[0;36mshape\u001b[1;34m(input, out_type, name)\u001b[0m\n\u001b[0;32m   9180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   9181\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 9182\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9183\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mShape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mout_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   9184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   9185\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dropout, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import os\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import functools\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "\n",
    "input_shape = (256, 256, 3)\n",
    "\n",
    "#매번바꿔주기\n",
    "num_classes = 205\n",
    "batch_size = 128\n",
    "\n",
    "initial_learning_rate = 0.048\n",
    "momentum = 0.9\n",
    "weight_decay = 0.00004\n",
    "dropout_rate = 0.2\n",
    "num_epochs = 50\n",
    "\n",
    "with strategy.scope():\n",
    "\n",
    "    # 데이터증강기법\n",
    "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1.0 / 255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        preprocessing_function=lambda img: tf.image.random_crop(img, [256, 256, 3])\n",
    "    )\n",
    "\n",
    "    # 훈련 데이터셋\n",
    "    train_directory = 'E:/종설/Training/1/[원천]면류'\n",
    "    train_dataset = train_datagen.flow_from_directory(\n",
    "        train_directory,\n",
    "        target_size=(256, 256),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    # 크기조정만\n",
    "    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1.0 / 255\n",
    "    )\n",
    "\n",
    "    # 검증데이터\n",
    "    val_directory = 'E:/종설/Validation/1/[원천]면류'\n",
    "    val_dataset = val_datagen.flow_from_directory(\n",
    "        val_directory,\n",
    "        target_size=(256, 256),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    # EfficientNet-B0모델 불러오기\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "    \n",
    "\n",
    "    # 층 추가\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    def lr_schedule(epoch):\n",
    "        initial_lr = initial_learning_rate\n",
    "        decay_factor = 0.97\n",
    "        decay_epochs = 2.4\n",
    "        lr = initial_lr * (decay_factor ** (epoch // decay_epochs))\n",
    "        return lr\n",
    "\n",
    "    # Define the optimizer with the learning rate schedule\n",
    "    optimizer = Adam(learning_rate=lr_schedule(0), beta_1=momentum, decay=weight_decay)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=len(train_dataset),\n",
    "        epochs=num_epochs,\n",
    "        callbacks=[lr_scheduler],\n",
    "        validation_data=val_dataset,\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    model.save('my_model_noodle_v4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21acf344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61cb0035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23370 images belonging to 205 classes.\n",
      "Found 3075 images belonging to 205 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[128,32,32,240] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_1/efficientnetb0/block3b_activation/mul-0-2-TransposeNCHWToNHWC-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_26792]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 65\u001b[0m\n\u001b[0;32m     56\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m val_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m     57\u001b[0m     val_directory,\n\u001b[0;32m     58\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m),\n\u001b[0;32m     59\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     60\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     61\u001b[0m )\n\u001b[0;32m     63\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Adjust this according to your needs\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:950\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    947\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;66;03m# stateless function.\u001b[39;00m\n\u001b[1;32m--> 950\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m   _, _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m    953\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    954\u001b[0m           \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1964\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[128,32,32,240] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_1/efficientnetb0/block3b_activation/mul-0-2-TransposeNCHWToNHWC-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_26792]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dropout, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import os\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import functools\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "\n",
    "\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)  # Adjust num_classes to your specific use case\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 데이터증강기법\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=lambda img: tf.image.random_crop(img, [256, 256, 3])\n",
    ")\n",
    "\n",
    "# 훈련 데이터셋\n",
    "train_directory = 'E:/종설/Training/1/[원천]면류'\n",
    "train_dataset = train_datagen.flow_from_directory(\n",
    "    train_directory,\n",
    "    target_size=(256, 256),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "# 크기조정만\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1.0 / 255\n",
    ")\n",
    "\n",
    "# 검증데이터\n",
    "val_directory = 'E:/종설/Validation/1/[원천]면류'\n",
    "val_dataset = val_datagen.flow_from_directory(\n",
    "    val_directory,\n",
    "    target_size=(256, 256),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "num_epochs = 10  # Adjust this according to your needs\n",
    "\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da82ec72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba78625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "256c4f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Found 23370 images belonging to 205 classes.\n",
      "Found 3075 images belonging to 205 classes.\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 256, 256, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name='input_3'), name='input_3', description=\"created by layer 'input_3'\"), but it was called on an input with incompatible shape (None, 224, 224, 3).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lr_scheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 87\u001b[0m\n\u001b[0;32m     78\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(inputs, outputs)\n\u001b[0;32m     80\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     83\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     84\u001b[0m     train_dataset,\n\u001b[0;32m     85\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataset),\n\u001b[0;32m     86\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mnum_epochs,\n\u001b[1;32m---> 87\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[\u001b[43mlr_scheduler\u001b[49m],\n\u001b[0;32m     88\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mval_dataset,\n\u001b[0;32m     89\u001b[0m )\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     92\u001b[0m best_val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lr_scheduler' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dropout, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import os\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import functools\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "\n",
    "input_shape = (256, 256, 3)\n",
    "\n",
    "#매번바꿔주기\n",
    "num_classes = 205\n",
    "batch_size = 128\n",
    "\n",
    "initial_learning_rate = 0.048\n",
    "momentum = 0.9\n",
    "weight_decay = 0.00004\n",
    "dropout_rate = 0.2\n",
    "num_epochs = 50\n",
    "\n",
    "with strategy.scope():\n",
    "\n",
    "    # 데이터증강기법\n",
    "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1.0 / 255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        preprocessing_function=lambda img: tf.image.random_crop(img, [256, 256, 3])\n",
    "    )\n",
    "\n",
    "    # 훈련 데이터셋\n",
    "    train_directory = 'E:/종설/Training/1/[원천]면류'\n",
    "    train_dataset = train_datagen.flow_from_directory(\n",
    "        train_directory,\n",
    "        target_size=(256, 256),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "    # 크기조정만\n",
    "    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1.0 / 255\n",
    "    )\n",
    "\n",
    "    # 검증데이터\n",
    "    val_directory = 'E:/종설/Validation/1/[원천]면류'\n",
    "    val_dataset = val_datagen.flow_from_directory(\n",
    "        val_directory,\n",
    "        target_size=(256, 256),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    # EfficientNet-B0모델 불러오기\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)  # Adjust num_classes to your specific use case\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "num_epochs = 10  # Adjust this according to your needs\n",
    "\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=num_epochs)\n",
    "\n",
    "    # Save the model\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    model.save('my_model_noodle_v4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f4404e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26892b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a038eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Found 23370 images belonging to 205 classes.\n",
      "Found 3075 images belonging to 205 classes.\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sj990\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sj990\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend.py:4846: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  13/1461 [..............................] - ETA: 36:31 - loss: 5.4919 - accuracy: 0.0192"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import os\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import functools\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    # 데이터증강기법\n",
    "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        shear_range=0.3,\n",
    "        zoom_range=0.3,\n",
    "        horizontal_flip=True,\n",
    "        preprocessing_function=lambda img: tf.image.random_crop(img, [256, 256, 3])\n",
    "    )\n",
    "\n",
    "    # 훈련 데이터셋\n",
    "    train_directory = 'E:/종설/Training/1/[원천]면류'\n",
    "    train_dataset = train_datagen.flow_from_directory(\n",
    "        train_directory,\n",
    "        target_size=(256, 256),\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        seed=42,  # add a seed for reproducibility\n",
    "        follow_links=True,  # follow symbolic links if any\n",
    "        classes=None,  # set to None to use default class mode\n",
    "    )\n",
    "\n",
    "    # 크기조정만\n",
    "    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "    )\n",
    "\n",
    "    # 검증데이터\n",
    "    val_directory = 'E:/종설/Validation/1/[원천]면류'\n",
    "    val_dataset = val_datagen.flow_from_directory(\n",
    "        val_directory,\n",
    "        target_size=(256, 256),\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        classes=None,  # set to None to use default class mode\n",
    "    )\n",
    "\n",
    "    # EfficientNet-B0모델 불러오기\n",
    "    pretrained_model = hub.KerasLayer(\"https://tfhub.dev/google/efficientnet/b0/feature-vector/1\")\n",
    "\n",
    "    # 층 추가\n",
    "    model = Sequential()\n",
    "    model.add(pretrained_model)\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(train_dataset.num_classes, activation='softmax', kernel_regularizer=l2(0.0003)))\n",
    "\n",
    "    # 컴파일 - 학습률조정\n",
    "    model.compile(\n",
    "        optimizer=RMSprop(lr=0.0005),  # Adam사용도 고려\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    num_epochs = 50\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=val_dataset,\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    model.save('my_model_noodle_exex.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1248fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'상품': '10111_오뚜기컵누들매콤37.8G', '정확도': 0.97}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# 사용한 이미지 파일 삭제 필요할시 주석 제거후 사용\\nif results != None :\\n    os.remove(image_path)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import glob     \n",
    "import json\n",
    "import os\n",
    "\n",
    "# 1. 모델 불러오기\n",
    "model = tf.keras.models.load_model('model_noodle.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "\n",
    "# 경로설정\n",
    "image_directory = r'C:\\Users\\sj990\\Desktop'\n",
    "\n",
    "# jpg, png, jpeg 형식 (리스트형식으로 저장된다, 경로에는 파일한장만 존재하게)\n",
    "image_files = glob.glob(image_directory + '\\*.png') + glob.glob(image_directory + '\\*.jpg') + glob.glob(image_directory + '\\*.jpeg')\n",
    "\n",
    "# 리스트중 첫번째것만 사용\n",
    "image_path = image_files[0]\n",
    "\n",
    "\n",
    "# 이미지전처리, 사이즈조정만\n",
    "img = tf.keras.preprocessing.image.load_img(image_path, target_size=(256, 256))\n",
    "img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img = img.astype('float32') / 255.0\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "# 이미지 예측\n",
    "preds = model.predict(img)\n",
    "\n",
    "class_labels = []\n",
    "\n",
    "# 3. txt파일 불러오기\n",
    "with open('model_noodle_label.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        class_labels.append(line.strip())\n",
    "\n",
    "# 결과값 상위 1개\n",
    "top_preds_idx = preds[0].argsort()[ : : -1][ : 1]\n",
    "top_preds_labels = [class_labels[idx] for idx in top_preds_idx]\n",
    "top_preds_probs = preds[0][top_preds_idx]\n",
    "\n",
    "results = []\n",
    "\n",
    "#json형식 results\n",
    "for label, prob in zip(top_preds_labels, top_preds_probs):\n",
    "    result = {\n",
    "\n",
    "        '상품': label,\n",
    "        '정확도': round(float(prob), 2)\n",
    "    }\n",
    "    results.append(result)\n",
    "        \n",
    "        \n",
    "'''\n",
    "# json파일 저장필요할시 주석 제거후 사용\n",
    "with open('results.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(results, file, ensure_ascii=False)\n",
    "'''\n",
    "\n",
    "\n",
    "#출력예시\n",
    "print(results)\n",
    "\n",
    "'''\n",
    "# 사용한 이미지 파일 삭제 필요할시 주석 제거후 사용\n",
    "if results != None :\n",
    "    os.remove(image_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34e8adb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
